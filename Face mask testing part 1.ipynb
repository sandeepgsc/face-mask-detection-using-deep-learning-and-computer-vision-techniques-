{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "excessive-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "following-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "working-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cubic-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "brief-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "refined-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxtpath = os.path.sep.join([r'C:\\Users\\korla\\Desktop\\projects_ME\\open CV\\Face Mask Detection\\saving path\\mobilenet_v2.model\\variables','deploy.prototxt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "possible-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightspath = os.path.sep.join([r'C:\\Users\\korla\\Desktop\\projects_ME\\open CV\\Face Mask Detection\\saving path\\mobilenet_v2.model\\variables','res10_300x300_ssd_iter_140000.caffemodel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "stainless-lightning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\korla\\\\Desktop\\\\projects_ME\\\\open CV\\\\Face Mask Detection\\\\saving path\\\\mobilenet_v2.model\\\\variables\\\\deploy.prototxt'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prototxtpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "blind-graphics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\korla\\\\Desktop\\\\projects_ME\\\\open CV\\\\Face Mask Detection\\\\saving path\\\\mobilenet_v2.model\\\\variables\\\\res10_300x300_ssd_iter_140000.caffemodel'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aboriginal-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(prototxtpath , weightspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "compressed-writing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dnn_Net 000001F8A29E9550>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "increased-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_model(r'C:\\Users\\korla\\Desktop\\projects_ME\\open CV\\Face Mask Detection\\saving path\\mobilenet_v2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "actual-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread(r'C:\\Users\\korla\\Desktop\\projects_ME\\open CV\\Face Mask Detection\\example\\0_0_0 copy 6.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "wanted-science",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 87,  92,  93],\n",
       "        [ 99, 104, 105],\n",
       "        [108, 113, 114],\n",
       "        ...,\n",
       "        [106, 116, 133],\n",
       "        [109, 119, 136],\n",
       "        [108, 118, 135]],\n",
       "\n",
       "       [[ 91,  96,  97],\n",
       "        [ 97, 102, 103],\n",
       "        [103, 108, 109],\n",
       "        ...,\n",
       "        [109, 117, 134],\n",
       "        [109, 119, 136],\n",
       "        [108, 118, 135]],\n",
       "\n",
       "       [[ 93, 101, 101],\n",
       "        [ 93, 101, 101],\n",
       "        [ 97, 105, 105],\n",
       "        ...,\n",
       "        [106, 115, 129],\n",
       "        [110, 119, 133],\n",
       "        [111, 119, 136]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 47,  33,  34],\n",
       "        [ 42,  28,  29],\n",
       "        [ 40,  27,  25],\n",
       "        ...,\n",
       "        [ 32,  23,  19],\n",
       "        [ 45,  36,  32],\n",
       "        [ 52,  43,  39]],\n",
       "\n",
       "       [[ 46,  32,  33],\n",
       "        [ 42,  28,  29],\n",
       "        [ 41,  28,  26],\n",
       "        ...,\n",
       "        [ 53,  44,  40],\n",
       "        [ 59,  50,  46],\n",
       "        [ 57,  48,  44]],\n",
       "\n",
       "       [[ 44,  30,  31],\n",
       "        [ 42,  28,  29],\n",
       "        [ 44,  31,  29],\n",
       "        ...,\n",
       "        [ 53,  42,  38],\n",
       "        [ 58,  47,  43],\n",
       "        [ 62,  51,  47]]], dtype=uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "coral-mouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551, 383, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "manufactured-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w = image1.shape[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "thousand-buffer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551, 383)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "chubby-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog = cv2.dnn.blobFromImage(image1,1.0,(300,300),(104.0,177.0,123.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "appointed-singapore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ -14.,   -3.,    2., ...,    0.,    4.,    4.],\n",
       "         [  -9.,   -8.,   -3., ...,   -4.,    1.,    3.],\n",
       "         [ -16.,  -16.,  -15., ...,   -9.,  -12.,  -11.],\n",
       "         ...,\n",
       "         [ -63.,  -64.,  -65., ...,  -76.,  -84.,  -74.],\n",
       "         [ -58.,  -62.,  -67., ...,  -75.,  -67.,  -57.],\n",
       "         [ -60.,  -62.,  -69., ...,  -56.,  -48.,  -45.]],\n",
       "\n",
       "        [[ -82.,  -71.,  -66., ...,  -64.,  -59.,  -59.],\n",
       "         [ -74.,  -73.,  -68., ...,  -69.,  -64.,  -62.],\n",
       "         [ -80.,  -80.,  -79., ...,  -74.,  -77.,  -77.],\n",
       "         ...,\n",
       "         [-152., -152., -153., ..., -158., -166., -153.],\n",
       "         [-146., -149., -154., ..., -157., -149., -139.],\n",
       "         [-147., -148., -155., ..., -138., -131., -128.]],\n",
       "\n",
       "        [[ -27.,  -16.,  -11., ...,    6.,   12.,   12.],\n",
       "         [ -20.,  -19.,  -14., ...,   -1.,    5.,    9.],\n",
       "         [ -23.,  -23.,  -22., ...,   -7.,  -10.,   -8.],\n",
       "         ...,\n",
       "         [ -97.,  -99., -101., ..., -108., -116., -104.],\n",
       "         [ -91.,  -96., -102., ..., -107.,  -99.,  -89.],\n",
       "         [ -92.,  -95., -103., ...,  -88.,  -81.,  -77.]]]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "intelligent-duration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 300, 300)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "continued-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.setInput(blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "serious-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = net.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "sought-therapy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.        , 1.        , 0.9999676 , ..., 1.9832222 ,\n",
       "          2.980241  , 2.9804547 ],\n",
       "         [0.        , 1.        , 0.99891233, ..., 2.0086453 ,\n",
       "          3.7700975 , 2.9615614 ],\n",
       "         [0.        , 1.        , 0.99842846, ..., 2.009103  ,\n",
       "          1.9831141 , 2.9634216 ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "offensive-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "prototxtPath=os.path.sep.join([r'C:\\Users\\korla\\Desktop\\projects_ME\\open CV\\Face Mask Detection\\saving path\\mobilenet_v2.model\\variables','deploy.prototxt'])\n",
    "weightsPath=os.path.sep.join([r'C:\\Users\\korla\\Desktop\\projects_ME\\open CV\\Face Mask Detection\\saving path\\mobilenet_v2.model\\variables','res10_300x300_ssd_iter_140000.caffemodel'])\n",
    "\n",
    "\n",
    "\n",
    "net=cv2.dnn.readNet(prototxtPath,weightsPath)\n",
    "\n",
    "\n",
    "model=load_model(r'C:\\Users\\korla\\Desktop\\projects_ME\\open CV\\Face Mask Detection\\saving path\\mobilenet_v2.model')\n",
    "\n",
    "image=cv2.imread(r'C:\\Users\\korla\\Desktop\\projects_ME\\open CV\\Face Mask Detection\\example\\with_mask237.jpg')\n",
    "\n",
    "\n",
    "(h,w)=image.shape[:2]\n",
    "\n",
    "\n",
    "blob=cv2.dnn.blobFromImage(image,1.0,(300,300),(104.0,177.0,123.0))\n",
    "\n",
    "\n",
    "\n",
    "net.setInput(blob)\n",
    "detections=net.forward()\n",
    "\n",
    "\n",
    "#loop over the detections\n",
    "for i in range(0,detections.shape[2]):\n",
    "    confidence=detections[0,0,i,2]\n",
    "    \n",
    "    \n",
    "    if confidence>0.5:\n",
    "        #we need the X,Y coordinates\n",
    "        box=detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "        (startX,startY,endX,endY)=box.astype('int')\n",
    "        \n",
    "        #ensure the bounding boxes fall within the dimensions of the frame\n",
    "        (startX,startY)=(max(0,startX),max(0,startY))\n",
    "        (endX,endY)=(min(w-1,endX), min(h-1,endY))\n",
    "        \n",
    "        \n",
    "        #extract the face ROI, convert it from BGR to RGB channel, resize it to 224,224 and preprocess it\n",
    "        face=image[startY:endY, startX:endX]\n",
    "        face=cv2.cvtColor(face,cv2.COLOR_BGR2RGB)\n",
    "        face=cv2.resize(face,(224,224))\n",
    "        face=img_to_array(face)\n",
    "        face=preprocess_input(face)\n",
    "        face=np.expand_dims(face,axis=0)\n",
    "        \n",
    "        (mask,withoutMask)=model.predict(face)[0]\n",
    "        \n",
    "        #determine the class label and color we will use to draw the bounding box and text\n",
    "        label='Mask' if mask>withoutMask else 'No Mask'\n",
    "        color=(0,255,0) if label=='Mask' else (0,0,255)\n",
    "        \n",
    "        #include the probability in the label\n",
    "        label=\"{}: {:.2f}%\".format(label,max(mask,withoutMask)*100)\n",
    "        \n",
    "        #display the label and bounding boxes\n",
    "        cv2.putText(image,label,(startX,startY-10),cv2.FONT_HERSHEY_SIMPLEX,0.45,color,2)\n",
    "        cv2.rectangle(image,(startX,startY),(endX,endY),color,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "cv2.imshow(\"OutPut\",image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-employer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
